{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Milestone 4: Deep learning, due Wednesday, April 26, 2017\n",
    "\n",
    "For this milestone you will (finally) use deep learning to predict movie genres. You will train one small network from scratch on the posters only, and compare this one to a pre-trained network that you fine tune. [Here](https://keras.io/getting-started/faq/#how-can-i-use-pre-trained-models-in-keras) is a description of how to use pretrained models in Keras.\n",
    "\n",
    "You can try different architectures, initializations, parameter settings, optimization methods, etc. Be adventurous and explore deep learning! It can be fun to combine the features learned by the deep learning model with a SVM, or incorporate meta data into your deep learning model. \n",
    "\n",
    "**Note:** Be mindful of the longer training times for deep models. Not only for training time, but also for the parameter tuning efforts. You need time to develop a feel for the different parameters and which settings work, which normalization you want to use, which model architecture you choose, etc. \n",
    "\n",
    "It is great that we have GPUs via AWS to speed up the actual computation time, but you need to be mindful of your AWS credits. The GPU instances are not cheap and can accumulate costs rather quickly. Think about your model first and do some quick dry runs with a larger learning rate or large batch size on your local machine. \n",
    "\n",
    "The notebook to submit this week should at least include:\n",
    "\n",
    "- Complete description of the deep network you trained from scratch, including parameter settings, performance, features learned, etc. \n",
    "- Complete description of the pre-trained network that you fine tuned, including parameter settings, performance, features learned, etc. \n",
    "- Discussion of the results, how much improvement you gained with fine tuning, etc. \n",
    "- Discussion of at least one additional exploratory idea you pursued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import PIL\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# for image manipulation. Easier to do \n",
    "# here than with Keras, as per\n",
    "# https://piazza.com/class/ivlbdd3nigy3um?cid=818\n",
    "#!sudo pip install Image\n",
    "import PIL.Image as Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step One: Extracting Movies From URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train = pd.read_csv(\"train_full.csv\")\n",
    "# train.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "# print \"Train shape:\", train.shape\n",
    "# train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_thinned shape: (540, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10402</th>\n",
       "      <th>10749</th>\n",
       "      <th>10751</th>\n",
       "      <th>10752</th>\n",
       "      <th>12</th>\n",
       "      <th>14</th>\n",
       "      <th>16</th>\n",
       "      <th>18</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>...</th>\n",
       "      <th>lead actors</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>poster_path</th>\n",
       "      <th>release_date</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[u'Alec Baldwin', u'Miles Bakshi', u'Jimmy Kim...</td>\n",
       "      <td>295693</td>\n",
       "      <td>A story about how a new baby's arrival impacts...</td>\n",
       "      <td>305.881041</td>\n",
       "      <td>/unPB1iyEeTBcKiLg8W083rlViFH.jpg</td>\n",
       "      <td>2017-03-23</td>\n",
       "      <td>The Boss Baby</td>\n",
       "      <td>False</td>\n",
       "      <td>5.7</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10402  10749  10751  10752  12  14  16  18  27  28    ...      \\\n",
       "0      0      0      1      0   0   0   1   0   0   0    ...       \n",
       "\n",
       "                                         lead actors  movie_id  \\\n",
       "0  [u'Alec Baldwin', u'Miles Bakshi', u'Jimmy Kim...    295693   \n",
       "\n",
       "                                            overview  popularity  \\\n",
       "0  A story about how a new baby's arrival impacts...  305.881041   \n",
       "\n",
       "                        poster_path  release_date          title  video  \\\n",
       "0  /unPB1iyEeTBcKiLg8W083rlViFH.jpg    2017-03-23  The Boss Baby  False   \n",
       "\n",
       "  vote_average vote_count  \n",
       "0          5.7        510  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_thinned = pd.read_csv(\"train.csv\")\n",
    "train_thinned.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "print \"train_thinned shape:\", train_thinned.shape\n",
    "train_thinned.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Important. \n",
    "\n",
    "The line below aliases the DF that we want to work with as `curr_df`. When we decide later on to use the full training set instead of just `train_thinned`, all we need to do is set it in the cell below and re-run the code. This will prevent us from having to find/replace all instances of the past dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "curr_df = train_thinned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Helper that downloads web images \n",
    "## Takes in the poster path and the id of the movie \n",
    "## Saves the movie as a jpg as the unique id of the movie \n",
    "## In the images folder.\n",
    "def download_web_image(poster_path, movie_id):\n",
    "    # given that we're going to resize our images to be 32x32\n",
    "    # or something else really small, let's download really small images \n",
    "    # to start\n",
    "    base_url = \"https://image.tmdb.org/t/p/w92/\" \n",
    "    \n",
    "    request = urllib2.Request(base_url + poster_path)\n",
    "    img = urllib2.urlopen(request).read()\n",
    "    image_name= \"images/\" + str(movie_id) + \".jpg\"\n",
    "    \n",
    "    with open(image_name, 'w') as f: \n",
    "        f.write(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you actually want to download posters, you'll need to turn the `1` above into a `0`. This code doesn't run by default in the notebook so that you don't accidentally download hundreds of images.\n"
     ]
    }
   ],
   "source": [
    "### iterate through all of the images in the thinned dataset, saving locally \n",
    "if 1:\n",
    "    print \"If you actually want to download posters, you'll need to turn the `1` above into a `0`. This code doesn't run by default in the notebook so that you don't accidentally download hundreds of images.\"\n",
    "else:\n",
    "    for index, row in curr_df.iterrows():\n",
    "        movie_id = row[\"movie_id\"]\n",
    "        poster_path = row[\"poster_path\"] \n",
    "#         download_web_image(poster_path, movie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "# convert each normal poster to a 32x32 grayscale poster\n",
    "for img_name in os.listdir(\"images/\"):\n",
    "    ## This line added to avoid hidden files on mac (Stephen)\n",
    "    if not img_name.startswith('.'):\n",
    "        # read in an image and convert to greyscale\n",
    "        im = Image.open(\"images/\" + img_name).convert(\"L\")\n",
    "        out = im.resize((img_rows, img_cols))\n",
    "        out.save(\"nn_ready_images/\" + img_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Building a CNN from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# number of labels in our output\n",
    "n_labels = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# now we need training and testing data. in the current state,\n",
    "# we have a bunch of greyscale images named by their movie ids.\n",
    "# to get the data, we can first just split all the movie ids (X) in the\n",
    "# dataframe intro train and test sets, and then grab their multilabel\n",
    "# matrices (y)\n",
    "m_ids = curr_df.movie_id.values\n",
    "\n",
    "# shuffle the ids to get a random sample\n",
    "np.random.shuffle(m_ids)\n",
    "\n",
    "import math\n",
    "train_size = int(math.floor(.7 * len(m_ids)))\n",
    "\n",
    "# get the movie_ids (each of which has an image in \"nn_images_ready/\"\n",
    "# which is ready to be put through the neural net\n",
    "train_ids = m_ids[:train_size]\n",
    "test_ids = m_ids[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (378, 17)\n",
      "y_test shape: (162, 17)\n"
     ]
    }
   ],
   "source": [
    "# these are the column names of the multilabel matrix\n",
    "label_names = curr_df.columns[:n_labels]\n",
    "\n",
    "y_train = np.array([curr_df[curr_df.movie_id == movie_id][label_names].values[0] for movie_id in train_ids])\n",
    "y_test  = np.array([curr_df[curr_df.movie_id == movie_id][label_names].values[0] for movie_id in test_ids])\n",
    "\n",
    "# should be (num_samples, num_labels)\n",
    "print \"y_train shape:\", y_train.shape\n",
    "print \"y_test shape:\", y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# smaller batch size means noisier gradient, but more updates per epoch\n",
    "batch_size = 512\n",
    "\n",
    "# number of iterations over the complete training data\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load image matrices into memory\n",
    "x_train = np.array([np.asarray(Image.open(\"nn_ready_images/\" + str(m_id) + \".jpg\")) for m_id in train_ids])\n",
    "x_test =  np.array([np.asarray(Image.open(\"nn_ready_images/\" + str(m_id) + \".jpg\")) for m_id in test_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (378, 32, 32)\n",
      "x_test shape: (162, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# output should be (num_images, img_height, img_width)\n",
    "print \"x_train shape:\", x_train.shape\n",
    "print \"x_test shape:\", x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 387072 into shape (378,299,299,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f5f9d8d129a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 387072 into shape (378,299,299,1)"
     ]
    }
   ],
   "source": [
    "# code borrowed from Keras_CNN lab\n",
    "\n",
    "# now we need to reshape x_train and x_test so that they work with CNNs\n",
    "# Following the example in \"labs/Keras_CNN.ipynb\", this needs to be an array \n",
    "# of images with shape determined by the backend, including the depth dimension,\n",
    "# which is 1 for greyscale\n",
    "\n",
    "# x_train is of shape n_samples x 32 x 32\n",
    "# for a CNN we want to keep the image shape\n",
    "# need to explicitly tell keras that it is a gray value image\n",
    "# so each image is 32x32x1 not 32x32x3\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "# normalize image values to [0,1]\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print \"x_train shape:\", x_train.shape\n",
    "print x_train.shape[0], \"train samples\"\n",
    "print x_test.shape[0], \"test samples\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_95 (Conv2D)           (None, 28, 28, 16)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_96 (Conv2D)           (None, 12, 12, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_97 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 17)                1105      \n",
      "=================================================================\n",
      "Total params: 41,105.0\n",
      "Trainable params: 41,105.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create an empty network model\n",
    "model = Sequential()\n",
    "\n",
    "# define the input layer to the CNN\n",
    "# input shape is a tuple of the # rows, # cols, and # channels (1 for grayscale)\n",
    "# the first parameter to Conv2D is the number of filters we want to convolve\n",
    "# over the input images\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
    "\n",
    "# create a max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add another convolution layer\n",
    "# we could double the number of filters as max pool made the \n",
    "# feature maps much smaller, but we're not doing this to improve runtime\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "# create a max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# ================\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "# create a max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# ================\n",
    "\n",
    "# flatten for fully connected classification layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# note that the 10 is the number of classes we have\n",
    "# the classes are mutually exclusive so softmax is a good choice\n",
    "# --- fully connected layer ---\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# recommended by: https://github.com/fchollet/keras/issues/761\n",
    "# uses a sigmoid activation rather than softmax, which apparently\n",
    "# gives us a label vector back\n",
    "model.add(Dense(n_labels, activation='sigmoid'))\n",
    "\n",
    "# prints out a summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "Let's use a large learning rate (0.1) while we're working locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the setup is our basic categorical crossentropy with stochastic gradient decent\n",
    "# we also specify that we want to evaluate our model in terms of accuracy\n",
    "sgd = SGD(lr=0.1, momentum=0.9)\n",
    "\n",
    "# TODO: why are we using binary crossentropy?\n",
    "# I'm not sure, but it works much better than\n",
    "# categorical crossentropy.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 378 samples, validate on 162 samples\n",
      "Epoch 1/30\n",
      "378/378 [==============================] - 1s - loss: 0.6857 - acc: 0.5893 - val_loss: 0.6808 - val_acc: 0.6264\n",
      "Epoch 2/30\n",
      "378/378 [==============================] - 0s - loss: 0.6793 - acc: 0.6513 - val_loss: 0.6685 - val_acc: 0.6743\n",
      "Epoch 3/30\n",
      "378/378 [==============================] - 0s - loss: 0.6670 - acc: 0.6945 - val_loss: 0.6497 - val_acc: 0.7019\n",
      "Epoch 4/30\n",
      "378/378 [==============================] - 0s - loss: 0.6486 - acc: 0.7163 - val_loss: 0.6224 - val_acc: 0.7190\n",
      "Epoch 5/30\n",
      "378/378 [==============================] - 1s - loss: 0.6216 - acc: 0.7155 - val_loss: 0.5816 - val_acc: 0.7858\n",
      "Epoch 6/30\n",
      "378/378 [==============================] - 1s - loss: 0.5815 - acc: 0.7786 - val_loss: 0.5246 - val_acc: 0.8199\n",
      "Epoch 7/30\n",
      "378/378 [==============================] - 1s - loss: 0.5252 - acc: 0.8008 - val_loss: 0.4715 - val_acc: 0.8199\n",
      "Epoch 8/30\n",
      "378/378 [==============================] - 0s - loss: 0.4715 - acc: 0.8013 - val_loss: 0.4665 - val_acc: 0.8210\n",
      "Epoch 9/30\n",
      "378/378 [==============================] - 1s - loss: 0.4594 - acc: 0.8038 - val_loss: 0.5116 - val_acc: 0.8293\n",
      "Epoch 10/30\n",
      "378/378 [==============================] - 1s - loss: 0.4826 - acc: 0.8265 - val_loss: 0.5728 - val_acc: 0.8021\n",
      "Epoch 11/30\n",
      "378/378 [==============================] - 0s - loss: 0.5131 - acc: 0.8120 - val_loss: 0.5785 - val_acc: 0.7821\n",
      "Epoch 12/30\n",
      "378/378 [==============================] - 0s - loss: 0.5106 - acc: 0.8059 - val_loss: 0.5040 - val_acc: 0.8086\n",
      "Epoch 13/30\n",
      "378/378 [==============================] - 0s - loss: 0.4607 - acc: 0.8171 - val_loss: 0.4541 - val_acc: 0.8290\n",
      "Epoch 14/30\n",
      "378/378 [==============================] - 0s - loss: 0.4408 - acc: 0.8265 - val_loss: 0.4467 - val_acc: 0.7898\n",
      "Epoch 15/30\n",
      "378/378 [==============================] - 0s - loss: 0.4482 - acc: 0.7723 - val_loss: 0.4313 - val_acc: 0.8010\n",
      "Epoch 16/30\n",
      "378/378 [==============================] - 0s - loss: 0.4358 - acc: 0.7812 - val_loss: 0.4233 - val_acc: 0.8315\n",
      "Epoch 17/30\n",
      "378/378 [==============================] - 0s - loss: 0.4256 - acc: 0.8284 - val_loss: 0.4244 - val_acc: 0.8083\n",
      "Epoch 18/30\n",
      "378/378 [==============================] - 2s - loss: 0.4230 - acc: 0.8181 - val_loss: 0.4256 - val_acc: 0.8061\n",
      "Epoch 19/30\n",
      "378/378 [==============================] - 1s - loss: 0.4203 - acc: 0.8145 - val_loss: 0.4253 - val_acc: 0.7988\n",
      "Epoch 20/30\n",
      "378/378 [==============================] - 1s - loss: 0.4166 - acc: 0.8106 - val_loss: 0.4264 - val_acc: 0.7988\n",
      "Epoch 21/30\n",
      "378/378 [==============================] - 1s - loss: 0.4149 - acc: 0.8112 - val_loss: 0.4292 - val_acc: 0.8199\n",
      "Epoch 22/30\n",
      "378/378 [==============================] - 1s - loss: 0.4158 - acc: 0.8190 - val_loss: 0.4305 - val_acc: 0.8199\n",
      "Epoch 23/30\n",
      "378/378 [==============================] - 0s - loss: 0.4161 - acc: 0.8190 - val_loss: 0.4280 - val_acc: 0.8199\n",
      "Epoch 24/30\n",
      "378/378 [==============================] - 0s - loss: 0.4131 - acc: 0.8193 - val_loss: 0.4238 - val_acc: 0.8293\n",
      "Epoch 25/30\n",
      "378/378 [==============================] - 0s - loss: 0.4088 - acc: 0.8265 - val_loss: 0.4207 - val_acc: 0.8290\n",
      "Epoch 26/30\n",
      "378/378 [==============================] - 0s - loss: 0.4059 - acc: 0.8266 - val_loss: 0.4195 - val_acc: 0.8290\n",
      "Epoch 27/30\n",
      "378/378 [==============================] - 0s - loss: 0.4051 - acc: 0.8265 - val_loss: 0.4193 - val_acc: 0.8293\n",
      "Epoch 28/30\n",
      "378/378 [==============================] - 1s - loss: 0.4054 - acc: 0.8265 - val_loss: 0.4186 - val_acc: 0.8293\n",
      "Epoch 29/30\n",
      "378/378 [==============================] - 1s - loss: 0.4051 - acc: 0.8265 - val_loss: 0.4169 - val_acc: 0.8293\n",
      "Epoch 30/30\n",
      "378/378 [==============================] - 1s - loss: 0.4036 - acc: 0.8265 - val_loss: 0.4149 - val_acc: 0.8293\n"
     ]
    }
   ],
   "source": [
    "# this is now the actual training\n",
    "# in addition to the training data we provide validation data\n",
    "# this data is used to calculate the performance of the model over all the epochs\n",
    "# this is useful to determine when training should stop\n",
    "# in our case we just use it to monitor the evolution of the model over the training epochs\n",
    "# if we use the validation data to determine when to stop the training or which model to save, we \n",
    "# should not use the test data, but a separate validation set. \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test loss:', 0.69265439480911062)\n",
      "('Test accuracy:', 0.54357300275637777)\n"
     ]
    }
   ],
   "source": [
    "# once training is complete, let's see how well we have done\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f719de98e90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAGDCAYAAAAs+rl+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngmBjvQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f719dedc910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here is a visualization of the training process\n",
    "# typically we gain a lot in the beginning and then\n",
    "# training slows down\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title(\"Accuracy as a function of epochs\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ToDos for #1\n",
    "1. Binary Categorization?\n",
    "2. Accuracy?\n",
    "3. Last layer for doing multilabel (is sigmoid correct?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Fine tuning a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Images to Format for InceptionV3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Because inception takes in a 299 x 299 RGB image, we need to download them as such \n",
    "img_rows, img_cols = 299, 299\n",
    "\n",
    "# convert each normal poster to a 299x299 poster\n",
    "for img_name in os.listdir(\"images/\"):\n",
    "    if not img_name.startswith('.'):\n",
    "        # read in an image, do not convert to greyscale \n",
    "        im = Image.open(\"images/\" + img_name)\n",
    "        out = im.resize((img_rows, img_cols))\n",
    "        ## save to the inception images folder \n",
    "        out.save(\"inception_ready_images/\" + img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load image matrices into memory\n",
    "x_train = np.array([np.asarray(Image.open(\"inception_ready_images/\" + str(m_id) + \".jpg\")) for m_id in train_ids])\n",
    "x_test =  np.array([np.asarray(Image.open(\"inception_ready_images/\" + str(m_id) + \".jpg\")) for m_id in test_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378, 299, 299, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Our data is of the format that we can use for inception \n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 299, 299, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## convert data into tuple of training data\n",
    "## model.fit_generator takes a tuple \n",
    "training = (x_train, y_train)\n",
    "\n",
    "test = (x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights= 'imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(n_labels, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "# model.fit_generator(x_train, y_train,\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=epochs,\n",
    "#                     verbose=1,\n",
    "#                     validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378, 299, 299, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## http://stackoverflow.com/questions/40574386/keras-model-fit-generator \n",
    "## This, in theory should generate the data in a way that we want. \n",
    "datagen = ImageDataGenerator()\n",
    "datagen.fit(x_train[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10/10 [==============================] - 7s - loss: 0.5146 - acc: 0.8118     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 7s - loss: 0.5304 - acc: 0.8059     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 6s - loss: 0.4656 - acc: 0.8176     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12cbd1c50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(x_train[0:20], y_train[0:20], batch_size = 1), steps_per_epoch = 10, epochs = 3)\n",
    "# # compute quantities required for featurewise normalization\n",
    "# # (std, mean, and principal components if ZCA whitening is applied)\n",
    "# datagen.fit(X_train)\n",
    "\n",
    "# # fits the model on batches with real-time data augmentation:\n",
    "# model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n",
    "#                     steps_per_epoch=len(X_train) / 32, epochs=epochs)\n",
    "\n",
    "# ## Might need to use this generator \n",
    "\n",
    "# train_datagen = ImageDataGenerator()\n",
    "\n",
    "# train_datagen.fit\n",
    "\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#         \"inception_ready_images/\",\n",
    "#         color_mode=\"grayscale\",\n",
    "#         target_size=(img_rows, img_cols),\n",
    "#         batch_size=1,\n",
    "#         class_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 378 samples, validate on 162 samples\n",
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "# # should not use the test data, but a separate validation set. \n",
    "# model.fit(x_train, y_train,\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=epochs,\n",
    "#                     verbose=1,\n",
    "#                     validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### See how many layers we want to freeze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'input_1')\n",
      "(1, 'conv2d_1')\n",
      "(2, 'batch_normalization_1')\n",
      "(3, 'activation_1')\n",
      "(4, 'conv2d_2')\n",
      "(5, 'batch_normalization_2')\n",
      "(6, 'activation_2')\n",
      "(7, 'conv2d_3')\n",
      "(8, 'batch_normalization_3')\n",
      "(9, 'activation_3')\n",
      "(10, 'max_pooling2d_1')\n",
      "(11, 'conv2d_4')\n",
      "(12, 'batch_normalization_4')\n",
      "(13, 'activation_4')\n",
      "(14, 'conv2d_5')\n",
      "(15, 'batch_normalization_5')\n",
      "(16, 'activation_5')\n",
      "(17, 'max_pooling2d_2')\n",
      "(18, 'conv2d_9')\n",
      "(19, 'batch_normalization_9')\n",
      "(20, 'activation_9')\n",
      "(21, 'conv2d_7')\n",
      "(22, 'conv2d_10')\n",
      "(23, 'batch_normalization_7')\n",
      "(24, 'batch_normalization_10')\n",
      "(25, 'activation_7')\n",
      "(26, 'activation_10')\n",
      "(27, 'average_pooling2d_1')\n",
      "(28, 'conv2d_6')\n",
      "(29, 'conv2d_8')\n",
      "(30, 'conv2d_11')\n",
      "(31, 'conv2d_12')\n",
      "(32, 'batch_normalization_6')\n",
      "(33, 'batch_normalization_8')\n",
      "(34, 'batch_normalization_11')\n",
      "(35, 'batch_normalization_12')\n",
      "(36, 'activation_6')\n",
      "(37, 'activation_8')\n",
      "(38, 'activation_11')\n",
      "(39, 'activation_12')\n",
      "(40, 'mixed0')\n",
      "(41, 'conv2d_16')\n",
      "(42, 'batch_normalization_16')\n",
      "(43, 'activation_16')\n",
      "(44, 'conv2d_14')\n",
      "(45, 'conv2d_17')\n",
      "(46, 'batch_normalization_14')\n",
      "(47, 'batch_normalization_17')\n",
      "(48, 'activation_14')\n",
      "(49, 'activation_17')\n",
      "(50, 'average_pooling2d_2')\n",
      "(51, 'conv2d_13')\n",
      "(52, 'conv2d_15')\n",
      "(53, 'conv2d_18')\n",
      "(54, 'conv2d_19')\n",
      "(55, 'batch_normalization_13')\n",
      "(56, 'batch_normalization_15')\n",
      "(57, 'batch_normalization_18')\n",
      "(58, 'batch_normalization_19')\n",
      "(59, 'activation_13')\n",
      "(60, 'activation_15')\n",
      "(61, 'activation_18')\n",
      "(62, 'activation_19')\n",
      "(63, 'mixed1')\n",
      "(64, 'conv2d_23')\n",
      "(65, 'batch_normalization_23')\n",
      "(66, 'activation_23')\n",
      "(67, 'conv2d_21')\n",
      "(68, 'conv2d_24')\n",
      "(69, 'batch_normalization_21')\n",
      "(70, 'batch_normalization_24')\n",
      "(71, 'activation_21')\n",
      "(72, 'activation_24')\n",
      "(73, 'average_pooling2d_3')\n",
      "(74, 'conv2d_20')\n",
      "(75, 'conv2d_22')\n",
      "(76, 'conv2d_25')\n",
      "(77, 'conv2d_26')\n",
      "(78, 'batch_normalization_20')\n",
      "(79, 'batch_normalization_22')\n",
      "(80, 'batch_normalization_25')\n",
      "(81, 'batch_normalization_26')\n",
      "(82, 'activation_20')\n",
      "(83, 'activation_22')\n",
      "(84, 'activation_25')\n",
      "(85, 'activation_26')\n",
      "(86, 'mixed2')\n",
      "(87, 'conv2d_28')\n",
      "(88, 'batch_normalization_28')\n",
      "(89, 'activation_28')\n",
      "(90, 'conv2d_29')\n",
      "(91, 'batch_normalization_29')\n",
      "(92, 'activation_29')\n",
      "(93, 'conv2d_27')\n",
      "(94, 'conv2d_30')\n",
      "(95, 'batch_normalization_27')\n",
      "(96, 'batch_normalization_30')\n",
      "(97, 'activation_27')\n",
      "(98, 'activation_30')\n",
      "(99, 'max_pooling2d_3')\n",
      "(100, 'mixed3')\n",
      "(101, 'conv2d_35')\n",
      "(102, 'batch_normalization_35')\n",
      "(103, 'activation_35')\n",
      "(104, 'conv2d_36')\n",
      "(105, 'batch_normalization_36')\n",
      "(106, 'activation_36')\n",
      "(107, 'conv2d_32')\n",
      "(108, 'conv2d_37')\n",
      "(109, 'batch_normalization_32')\n",
      "(110, 'batch_normalization_37')\n",
      "(111, 'activation_32')\n",
      "(112, 'activation_37')\n",
      "(113, 'conv2d_33')\n",
      "(114, 'conv2d_38')\n",
      "(115, 'batch_normalization_33')\n",
      "(116, 'batch_normalization_38')\n",
      "(117, 'activation_33')\n",
      "(118, 'activation_38')\n",
      "(119, 'average_pooling2d_4')\n",
      "(120, 'conv2d_31')\n",
      "(121, 'conv2d_34')\n",
      "(122, 'conv2d_39')\n",
      "(123, 'conv2d_40')\n",
      "(124, 'batch_normalization_31')\n",
      "(125, 'batch_normalization_34')\n",
      "(126, 'batch_normalization_39')\n",
      "(127, 'batch_normalization_40')\n",
      "(128, 'activation_31')\n",
      "(129, 'activation_34')\n",
      "(130, 'activation_39')\n",
      "(131, 'activation_40')\n",
      "(132, 'mixed4')\n",
      "(133, 'conv2d_45')\n",
      "(134, 'batch_normalization_45')\n",
      "(135, 'activation_45')\n",
      "(136, 'conv2d_46')\n",
      "(137, 'batch_normalization_46')\n",
      "(138, 'activation_46')\n",
      "(139, 'conv2d_42')\n",
      "(140, 'conv2d_47')\n",
      "(141, 'batch_normalization_42')\n",
      "(142, 'batch_normalization_47')\n",
      "(143, 'activation_42')\n",
      "(144, 'activation_47')\n",
      "(145, 'conv2d_43')\n",
      "(146, 'conv2d_48')\n",
      "(147, 'batch_normalization_43')\n",
      "(148, 'batch_normalization_48')\n",
      "(149, 'activation_43')\n",
      "(150, 'activation_48')\n",
      "(151, 'average_pooling2d_5')\n",
      "(152, 'conv2d_41')\n",
      "(153, 'conv2d_44')\n",
      "(154, 'conv2d_49')\n",
      "(155, 'conv2d_50')\n",
      "(156, 'batch_normalization_41')\n",
      "(157, 'batch_normalization_44')\n",
      "(158, 'batch_normalization_49')\n",
      "(159, 'batch_normalization_50')\n",
      "(160, 'activation_41')\n",
      "(161, 'activation_44')\n",
      "(162, 'activation_49')\n",
      "(163, 'activation_50')\n",
      "(164, 'mixed5')\n",
      "(165, 'conv2d_55')\n",
      "(166, 'batch_normalization_55')\n",
      "(167, 'activation_55')\n",
      "(168, 'conv2d_56')\n",
      "(169, 'batch_normalization_56')\n",
      "(170, 'activation_56')\n",
      "(171, 'conv2d_52')\n",
      "(172, 'conv2d_57')\n",
      "(173, 'batch_normalization_52')\n",
      "(174, 'batch_normalization_57')\n",
      "(175, 'activation_52')\n",
      "(176, 'activation_57')\n",
      "(177, 'conv2d_53')\n",
      "(178, 'conv2d_58')\n",
      "(179, 'batch_normalization_53')\n",
      "(180, 'batch_normalization_58')\n",
      "(181, 'activation_53')\n",
      "(182, 'activation_58')\n",
      "(183, 'average_pooling2d_6')\n",
      "(184, 'conv2d_51')\n",
      "(185, 'conv2d_54')\n",
      "(186, 'conv2d_59')\n",
      "(187, 'conv2d_60')\n",
      "(188, 'batch_normalization_51')\n",
      "(189, 'batch_normalization_54')\n",
      "(190, 'batch_normalization_59')\n",
      "(191, 'batch_normalization_60')\n",
      "(192, 'activation_51')\n",
      "(193, 'activation_54')\n",
      "(194, 'activation_59')\n",
      "(195, 'activation_60')\n",
      "(196, 'mixed6')\n",
      "(197, 'conv2d_65')\n",
      "(198, 'batch_normalization_65')\n",
      "(199, 'activation_65')\n",
      "(200, 'conv2d_66')\n",
      "(201, 'batch_normalization_66')\n",
      "(202, 'activation_66')\n",
      "(203, 'conv2d_62')\n",
      "(204, 'conv2d_67')\n",
      "(205, 'batch_normalization_62')\n",
      "(206, 'batch_normalization_67')\n",
      "(207, 'activation_62')\n",
      "(208, 'activation_67')\n",
      "(209, 'conv2d_63')\n",
      "(210, 'conv2d_68')\n",
      "(211, 'batch_normalization_63')\n",
      "(212, 'batch_normalization_68')\n",
      "(213, 'activation_63')\n",
      "(214, 'activation_68')\n",
      "(215, 'average_pooling2d_7')\n",
      "(216, 'conv2d_61')\n",
      "(217, 'conv2d_64')\n",
      "(218, 'conv2d_69')\n",
      "(219, 'conv2d_70')\n",
      "(220, 'batch_normalization_61')\n",
      "(221, 'batch_normalization_64')\n",
      "(222, 'batch_normalization_69')\n",
      "(223, 'batch_normalization_70')\n",
      "(224, 'activation_61')\n",
      "(225, 'activation_64')\n",
      "(226, 'activation_69')\n",
      "(227, 'activation_70')\n",
      "(228, 'mixed7')\n",
      "(229, 'conv2d_73')\n",
      "(230, 'batch_normalization_73')\n",
      "(231, 'activation_73')\n",
      "(232, 'conv2d_74')\n",
      "(233, 'batch_normalization_74')\n",
      "(234, 'activation_74')\n",
      "(235, 'conv2d_71')\n",
      "(236, 'conv2d_75')\n",
      "(237, 'batch_normalization_71')\n",
      "(238, 'batch_normalization_75')\n",
      "(239, 'activation_71')\n",
      "(240, 'activation_75')\n",
      "(241, 'conv2d_72')\n",
      "(242, 'conv2d_76')\n",
      "(243, 'batch_normalization_72')\n",
      "(244, 'batch_normalization_76')\n",
      "(245, 'activation_72')\n",
      "(246, 'activation_76')\n",
      "(247, 'max_pooling2d_4')\n",
      "(248, 'mixed8')\n",
      "(249, 'conv2d_81')\n",
      "(250, 'batch_normalization_81')\n",
      "(251, 'activation_81')\n",
      "(252, 'conv2d_78')\n",
      "(253, 'conv2d_82')\n",
      "(254, 'batch_normalization_78')\n",
      "(255, 'batch_normalization_82')\n",
      "(256, 'activation_78')\n",
      "(257, 'activation_82')\n",
      "(258, 'conv2d_79')\n",
      "(259, 'conv2d_80')\n",
      "(260, 'conv2d_83')\n",
      "(261, 'conv2d_84')\n",
      "(262, 'average_pooling2d_8')\n",
      "(263, 'conv2d_77')\n",
      "(264, 'batch_normalization_79')\n",
      "(265, 'batch_normalization_80')\n",
      "(266, 'batch_normalization_83')\n",
      "(267, 'batch_normalization_84')\n",
      "(268, 'conv2d_85')\n",
      "(269, 'batch_normalization_77')\n",
      "(270, 'activation_79')\n",
      "(271, 'activation_80')\n",
      "(272, 'activation_83')\n",
      "(273, 'activation_84')\n",
      "(274, 'batch_normalization_85')\n",
      "(275, 'activation_77')\n",
      "(276, 'mixed9_0')\n",
      "(277, 'concatenate_1')\n",
      "(278, 'activation_85')\n",
      "(279, 'mixed9')\n",
      "(280, 'conv2d_90')\n",
      "(281, 'batch_normalization_90')\n",
      "(282, 'activation_90')\n",
      "(283, 'conv2d_87')\n",
      "(284, 'conv2d_91')\n",
      "(285, 'batch_normalization_87')\n",
      "(286, 'batch_normalization_91')\n",
      "(287, 'activation_87')\n",
      "(288, 'activation_91')\n",
      "(289, 'conv2d_88')\n",
      "(290, 'conv2d_89')\n",
      "(291, 'conv2d_92')\n",
      "(292, 'conv2d_93')\n",
      "(293, 'average_pooling2d_9')\n",
      "(294, 'conv2d_86')\n",
      "(295, 'batch_normalization_88')\n",
      "(296, 'batch_normalization_89')\n",
      "(297, 'batch_normalization_92')\n",
      "(298, 'batch_normalization_93')\n",
      "(299, 'conv2d_94')\n",
      "(300, 'batch_normalization_86')\n",
      "(301, 'activation_88')\n",
      "(302, 'activation_89')\n",
      "(303, 'activation_92')\n",
      "(304, 'activation_93')\n",
      "(305, 'batch_normalization_94')\n",
      "(306, 'activation_86')\n",
      "(307, 'mixed9_1')\n",
      "(308, 'concatenate_2')\n",
      "(309, 'activation_94')\n",
      "(310, 'mixed10')\n"
     ]
    }
   ],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In total we have 310 layers prebuilt, we'll freeze everything but the last 2 in order to do our finetuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 172 layers and unfreeze the rest:\n",
    "for layer in model.layers[:308]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[308:]:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "### Lr is the learning rate, this is set currently to be relatively low \n",
    "### however, if we wanted to learn more quickly on each update we would increase this \n",
    "### THIS IS SOMETHING THAT WE CAN TEST \n",
    "## The other thing that we can check out is momentum , momentum is how much \n",
    "### the model continues to learn in the same direction. This is another model that we could check to see how \n",
    "## important it is via cross validation potentially. \n",
    "## Explanation is here http://sebastianruder.com/optimizing-gradient-descent/index.html#momentum \n",
    "model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss='binary_crossentropy', metric = [\"accuracy\"])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "#model.fit_generator(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Discussion of the results, how much improvement you gained with fine tuning, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. Discussion of at least one additional exploratory idea you pursued "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
