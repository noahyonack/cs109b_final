{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3: Traditional statistical and machine learning methods, due Wednesday, April 19, 2017\n",
    "\n",
    "Think about how you would address the genre prediction problem with traditional statistical or machine learning methods. This includes everything you learned about modeling in this course before the deep learning part. Implement your ideas and compare different classifiers. Report your results and discuss what challenges you faced and how you overcame them. What works and what does not? If there are parts that do not work as expected, make sure to discuss briefly what you think is the cause and how you would address this if you would have more time and resources. \n",
    "\n",
    "You do not necessarily need to use the movie posters for this step, but even without a background in computer vision, there are very simple features you can extract from the posters to help guide a traditional machine learning model. Think about the PCA lecture for example, or how to use clustering to extract color information. In addition to considering the movie posters it would be worthwhile to have a look at the metadata that IMDb provides. \n",
    "\n",
    "You could use Spark and the [ML library](https://spark.apache.org/docs/latest/ml-features.html#word2vec) to build your model features from the data. This may be especially beneficial if you use additional data, e.g., in text form.\n",
    "\n",
    "You also need to think about how you are going to evaluate your classifier. Which metrics or scores will you report to show how good the performance is?\n",
    "\n",
    "The notebook to submit this week should at least include:\n",
    "\n",
    "- Detailed description and implementation of two different models\n",
    "- Description of your performance metrics\n",
    "- Careful performance evaluations for both models\n",
    "- Visualizations of the metrics for performance evaluation\n",
    "- Discussion of the differences between the models, their strengths, weaknesses, etc. \n",
    "- Discussion of the performances you achieved, and how you might be able to improve them in the future\n",
    "\n",
    "#### Preliminary Peer Assessment\n",
    "\n",
    "It is important to provide positive feedback to people who truly worked hard for the good of the team and to also make suggestions to those you perceived not to be working as effectively on team tasks. We ask you to provide an honest assessment of the contributions of the members of your team, including yourself. The feedback you provide should reflect your judgment of each team member’s:\n",
    "\n",
    "- Preparation – were they prepared during team meetings?\n",
    "- Contribution – did they contribute productively to the team discussion and work?\n",
    "- Respect for others’ ideas – did they encourage others to contribute their ideas?\n",
    "- Flexibility – were they flexible when disagreements occurred?\n",
    "\n",
    "Your teammate’s assessment of your contributions and the accuracy of your self-assessment will be considered as part of your overall project score.\n",
    "\n",
    "Preliminary Peer Assessment: [https://goo.gl/forms/WOYC7pwRCSU0yV3l1](https://goo.gl/forms/WOYC7pwRCSU0yV3l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to answer: \n",
    "\n",
    "- What are we predicting exactly?\n",
    "So, we are trying to predict movie genres. However, we have that each movie has multiple genres. This leads to the question of how we can predict multiple classifiers for the same object. This more general question is called a multilabel clasification problem. We will explore some of our specifications for this problem below. \n",
    "\n",
    "One of the best and most standard solution to do multilable classification is called \"one vs. rest' classifiers. These classifiers create a n number of models for each of the differnet classifiers. One of the advantages of this model is its interpretability and, for our cases, its ease. We can easily create a pipeline that then does these predictions for us. For an implementation of onevsall, look at scikit learn http://scikit-learn.org/dev/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier\n",
    "\n",
    "We will likely be using this in our early attempts at classification. \n",
    "\n",
    "- What does it means to be succesful? What is our metric for success? \n",
    "\n",
    "*adapted from http://people.oregonstate.edu/~sorowerm/pdf/Qual-Multilabel-Shahed-CompleteVersion.pdf*\n",
    "\n",
    "A few options for our measure of accuracy.\n",
    "\n",
    "#### Exact Match Ratio\n",
    "The exact match ratio only considers a correct answer for our multilabel data if it is exactly correct (e.g. if there are three classes, we only classify this as correct if we correctly identify all three classes.) \n",
    "\n",
    "#### Accuracy \n",
    "Accuracy is a simple way of \"goodness of prediction.\" It is defined as follows \n",
    "\n",
    "$$ \\frac{1}{n} \\sum_i^{n}  \\frac{|Y_i\\cap Z_i|}{|Y_i \\cup Z_i|}$$\n",
    "\n",
    "Where $$Y_i\\cap Z_i $$ refers to the total number of correctly predicted labels over the total number of labels for that instance. So, if for example we predicted [romance, action]  and the true labels were [romance, comedy, horror], this would receive an accuracy of 1/4 because there was one correct prediction and 4 unique labels. \n",
    "\n",
    "\n",
    "#### Hamming Loss \n",
    "The final and most common form of error for multilable predictions is hamming loss. Hamming loss takes into account both the prediction error (an incorrect error is predicted) and the missing error (a relevant lable is NOT predicted.) this is defined as follows below \n",
    "\n",
    "$$ HammingLoss, HL = \\frac{1}{kn} \\sum_{i}^{n} \\sum_l^k [l \\in  Z_i \\wedge l \\notin Y_i)  + I(l \\notin Z_i \\wedge  l \\in Y_i)]$$\n",
    "\n",
    "For this project, we will use the hamming loss which is defined above. There is a convenient function in python to calculate hamming loss, this is shown below. \n",
    "\n",
    "- What is our first modeling approach? Why? \n",
    "\n",
    "- What is our second modeling approach? Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Showing hamming loss in action (delete this when submitting report)\n",
    "np.array([[0, 1], [1, 1]]), np.zeros((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_metadata = pd.read_csv(\"df_metadata.csv\", sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
