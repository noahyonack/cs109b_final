{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras import regularizers\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_line_patch(label):\n",
    "    patch = np.zeros((3,3))\n",
    "    if label == 0:\n",
    "        patch[:,1] = 1\n",
    "    else:\n",
    "        patch[1,:] = 1\n",
    "        \n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training data\n",
    "\n",
    "def generate_data(n_samples):\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    for i in range(n_samples):\n",
    "        y = (np.random.random(1)>0.5) * 1.0\n",
    "        x = generate_line_patch(y)\n",
    "        \n",
    "        data_x.append(x.flatten())\n",
    "        data_y.append(y)\n",
    "        \n",
    "    data_x = np.array(data_x)\n",
    "    data_y = np.array(data_y)\n",
    "    \n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_x_train, data_y_train = generate_data(100)\n",
    "data_x_test, data_y_test = generate_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 111\n",
      "Trainable params: 111\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(9,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.01)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 0s - loss: 0.5020 - acc: 0.8200 - val_loss: 0.3601 - val_acc: 1.0000\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s - loss: 0.2894 - acc: 1.0000 - val_loss: 0.2067 - val_acc: 1.0000\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s - loss: 0.1645 - acc: 1.0000 - val_loss: 0.1154 - val_acc: 1.0000\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s - loss: 0.0924 - acc: 1.0000 - val_loss: 0.0690 - val_acc: 1.0000\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s - loss: 0.0571 - acc: 1.0000 - val_loss: 0.0452 - val_acc: 1.0000\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s - loss: 0.0384 - acc: 1.0000 - val_loss: 0.0307 - val_acc: 1.0000\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s - loss: 0.0265 - acc: 1.0000 - val_loss: 0.0220 - val_acc: 1.0000\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s - loss: 0.0194 - acc: 1.0000 - val_loss: 0.0164 - val_acc: 1.0000\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s - loss: 0.0148 - acc: 1.0000 - val_loss: 0.0132 - val_acc: 1.0000\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s - loss: 0.0125 - acc: 1.0000 - val_loss: 0.0119 - val_acc: 1.0000\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s - loss: 0.0116 - acc: 1.0000 - val_loss: 0.0113 - val_acc: 1.0000\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s - loss: 0.0112 - acc: 1.0000 - val_loss: 0.0110 - val_acc: 1.0000\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s - loss: 0.0109 - acc: 1.0000 - val_loss: 0.0107 - val_acc: 1.0000\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s - loss: 0.0106 - acc: 1.0000 - val_loss: 0.0104 - val_acc: 1.0000\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s - loss: 0.0103 - acc: 1.0000 - val_loss: 0.0102 - val_acc: 1.0000\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s - loss: 0.0101 - acc: 1.0000 - val_loss: 0.0099 - val_acc: 1.0000\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s - loss: 0.0098 - acc: 1.0000 - val_loss: 0.0097 - val_acc: 1.0000\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s - loss: 0.0096 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s - loss: 0.0094 - acc: 1.0000 - val_loss: 0.0093 - val_acc: 1.0000\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s - loss: 0.0092 - acc: 1.0000 - val_loss: 0.0091 - val_acc: 1.0000\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0089 - val_acc: 1.0000\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s - loss: 0.0088 - acc: 1.0000 - val_loss: 0.0087 - val_acc: 1.0000\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0089 - acc: 1.000 - 0s - loss: 0.0086 - acc: 1.0000 - val_loss: 0.0085 - val_acc: 1.0000\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s - loss: 0.0085 - acc: 1.0000 - val_loss: 0.0084 - val_acc: 1.0000\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s - loss: 0.0083 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s - loss: 0.0081 - acc: 1.0000 - val_loss: 0.0080 - val_acc: 1.0000\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s - loss: 0.0080 - acc: 1.0000 - val_loss: 0.0079 - val_acc: 1.0000\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s - loss: 0.0078 - acc: 1.0000 - val_loss: 0.0077 - val_acc: 1.0000\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s - loss: 0.0075 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s - loss: 0.0069 - acc: 1.0000 - val_loss: 0.0068 - val_acc: 1.0000\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s - loss: 0.0065 - acc: 1.0000 - val_loss: 0.0064 - val_acc: 1.0000\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s - loss: 0.0064 - acc: 1.0000 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s - loss: 0.0062 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0059 - val_acc: 1.0000\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s - loss: 0.0059 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s - loss: 0.0056 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data_x_train, data_y_train,\n",
    "                    batch_size=10,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_data=(data_x_test, data_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAADQCAYAAABLPxt+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADrVJREFUeJzt3X9olfX7x/HX3A/nHDakMxLGbEJCoSJmmeDS0DWLxArD\nOTlSWpBKy7SYzm2uHzolkmog88cf4ZxpqaRZuNQkSUHKVDazIlFjaraZMWfqzs453z9i+/i1jzuL\nz7nud7t9Pv7qeOBc1znOnrvPzu47IRqNRgUAAOKul+sFAADwKyILAIARIgsAgBEiCwCAkaR4Pti1\na9fU0NCgQCCgxMTEeD40AAD/OuFwWE1NTRoyZIhSU1P/dn9cI9vQ0KDp06fH8yEBAPjXq62t1ciR\nI//253GNbCAQkCRlZWUpOTk5ng+Nf4G2tjYnc1NSUpzMdWXdunWez3z++ec9n+nSwoULncxdvny5\nk7kunD592sncu+++29N5oVBIjY2Nnf27WVwj2/EWcXJyMpH1oUgk4mTu7fa1lJWV5fnM2+01zszM\ndDL3dnqdExISnMx19Rrf6kekfPAJAAAjRBYAACNEFgAAI0QWAAAjRBYAACNEFgAAI0QWAAAjRBYA\nACNEFgAAI0QWAAAjMU+rGIlEVFFRoR9//FEpKSl66623NHDgQC92AwCgR4t5JLtnzx61tbVp8+bN\nWrBgwW11gmsAAP4XMSN7+PBh5ebmSpKGDx+uhoYG86UAAPCDmJFtbW1Venp65+3ExES1t7ebLgUA\ngB/EjGx6erquXLnSeTsSiSgpKa5XyAMAwJdiRnbEiBHav3+/JOno0aMaPHiw+VIAAPhBzEPSvLw8\nHThwQAUFBYpGo1q2bJkXewEA0OPFjGyvXr30xhtveLELAAC+wskoAAAwQmQBADBCZAEAMEJkAQAw\nQmQBADBCZAEAMEJkAQAwQmQBADBCZAEAMMKZ/nugQCDgZG5zc7OTuZFIxMncL774wsncrVu3ej6z\nvr7e85mSNHToUCdz169f72RuS0uLk7lz5szxfObu3bs9nynJ8/PrX758WadOnbrl/RzJAgBghMgC\nAGCEyAIAYITIAgBghMgCAGCEyAIAYITIAgBghMgCAGCEyAIAYITIAgBghMgCAGCkW5E9duyYgsGg\n9S4AAPhKzAsErF27Vjt27FCfPn282AcAAN+IeSSbnZ2tqqoqL3YBAMBXYkY2Pz9fSUlcEQ8AgH+K\nDz4BAGCEyAIAYITIAgBgpFuRzcrK0kcffWS9CwAAvsKRLAAARogsAABGiCwAAEaILAAARogsAABG\niCwAAEaILAAARogsAABGiCwAAEaILAAARriGXQ/U1NTkZG44HHYyNzEx0cnc/Px8J3Ozs7M9nzl0\n6FDPZ7o0ZswYJ3OPHz/uZO6GDRs8n/nTTz95PlOSLly44Om8UCjU5f0cyQIAYITIAgBghMgCAGCE\nyAIAYITIAgBghMgCAGCEyAIAYITIAgBghMgCAGCEyAIAYITIAgBgpMtzF4dCIZWUlOjs2bNqa2vT\n7NmzNX78eK92AwCgR+sysjt27FBGRobefvtt/fHHH3ryySeJLAAA3dRlZCdOnNh5JZJoNOrsaigA\nAPREXUa2b9++kqTW1lYVFRVp3rx5niwFAIAfxPzg0/nz5zVjxgxNnjxZkyZN8mInAAB8ocsj2ebm\nZs2cOVPl5eUaPXq0VzsBAOALXR7JVldXq6WlRatWrVIwGFQwGNS1a9e82g0AgB6tyyPZ0tJSlZaW\nerULAAC+wskoAAAwQmQBADBCZAEAMEJkAQAwQmQBADBCZAEAMEJkAQAwQmQBADBCZAEAMNLlGZ96\nkp07dzqZ+8QTTziZ60JeXp6TuV9++aWTua7MmTPH85l33XWX5zNdevXVV53Mveeee5zMdSEtLc31\nCv8KHMkCAGCEyAIAYITIAgBghMgCAGCEyAIAYITIAgBghMgCAGCEyAIAYITIAgBghMgCAGCEyAIA\nYITIAgBgJOYFAsLhsEpLS3Xq1CklJiaqsrJS2dnZXuwGAECPFvNIdt++fZKkTZs2qaioSJWVleZL\nAQDgBzGPZCdMmKBx48ZJks6dO6c777zTeicAAHyhW9eTTUpKUnFxsXbv3q3333/feicAAHyh2x98\nWrFiherq6lRWVqY///zTcicAAHwhZmQ/+eQTrV69WpLUp08fJSQkqFcvPpQMAEAsMd8ufvTRR7Vo\n0SJNnz5d7e3tKikpUWpqqhe7AQDQo8WMbFpamt577z0vdgEAwFd43xcAACNEFgAAI0QWAAAjRBYA\nACNEFgAAI0QWAAAjRBYAACNEFgAAI0QWAAAj3boKzz+Vn5+vjIwMi4e+peTkZE/ndZg0aZLnM19/\n/XXPZ0pydpnD+vp6J3OrqqqczO24hrOXXJ0qddSoUU7mbt++3cncCRMmOJkbiUQ8n/nLL794PlOS\nUlJSPJ13+fJlnTp16pb3cyQLAIARIgsAgBEiCwCAESILAIARIgsAgBEiCwCAESILAIARIgsAgBEi\nCwCAESILAIARIgsAgBEiCwCAkW5F9uLFixo7dqxOnjxpvQ8AAL4RM7KhUEjl5eXOrtIBAEBPFTOy\nK1asUEFBgTIzM73YBwAA3+gystu2bVP//v2Vm5vr1T4AAPhGl5HdunWrDh48qGAwqBMnTqi4uFhN\nTU1e7QYAQI+W1NWdtbW1nf8dDAZVUVGhQCBgvhQAAH7Ar/AAAGCkyyPZG9XU1FjuAQCA73AkCwCA\nESILAIARIgsAgBEiCwCAESILAIARIgsAgBEiCwCAESILAIARIgsAgJFun/Hpn6irq1NycrLFQ9/S\nzp07PZ3XIRQKeT5z6NChns+UpGHDhjmZ29zc7GTumjVrnMytq6vzfGZ+fr7nMyWpvr7eydwjR444\nmXs7XWBl7969TuaOHz/e03mxGsCRLAAARogsAABGiCwAAEaILAAARogsAABGiCwAAEaILAAARogs\nAABGiCwAAEaILAAARogsAABGunXu4qeeekrp6emSpKysLFVWVpouBQCAH8SM7PXr1xWNRlVTU+PF\nPgAA+EbMt4t/+OEHXb16VTNnztSMGTN09OhRL/YCAKDHi3kkm5qaqlmzZumZZ57R6dOn9cILL2jX\nrl1KSjK5Sh4AAL4Rs5Q5OTkaOHCgEhISlJOTo4yMDDU1NWnAgAFe7AcAQI8V8+3iLVu2aPny5ZKk\nCxcuqLW1VYFAwHwxAAB6uphHslOmTNGiRYs0bdo0JSQkaNmyZbxVDABAN8SsZUpKit555x0vdgEA\nwFc4GQUAAEaILAAARogsAABGiCwAAEaILAAARogsAABGiCwAAEaILAAARogsAABG4np+xHA4LEkK\nhULxfNhucXWqRxfP1ZWWlhYnc2+n11hy87V8u73GCQkJTubeTq9zYmKik7lev8Yd8zr6d7OEaDQa\njdewb7/9VtOnT4/XwwEA0CPU1tZq5MiRf/vzuEb22rVramhoUCAQcPZdDAAAXgmHw2pqatKQIUOU\nmpr6t/vjGlkAAPAffPAJAAAjRBYAACNEFgAAI0QWAAAjRBYAACPOIxuJRFReXq6pU6cqGAzqzJkz\nrlcyFQqF9Nprr6mwsFBTpkzR3r17Xa9k7uLFixo7dqxOnjzpehVzq1ev1tSpU/X000/r448/dr2O\nmVAopAULFqigoECFhYW+/rs9duyYgsGgJOnMmTOaNm2aCgsLtWTJEkUiEcfbxdeNz/XEiRMqLCxU\nMBjUrFmz1Nzc7Hi7+Lvx+Xb49NNPNXXq1LjNcB7ZPXv2qK2tTZs3b9aCBQu0fPly1yuZ2rFjhzIy\nMrRx40atW7dOb775puuVTIVCIZWXl//X3x/zm0OHDunIkSP68MMPVVNTo19//dX1Sma++uortbe3\na9OmTZo7d67effdd1yuZWLt2rUpLS3X9+nVJUmVlpebNm6eNGzcqGo366pvkm5/r0qVLVVZWppqa\nGuXl5Wnt2rWON4yvm5+vJH3//ffasmWL4vmbrc4je/jwYeXm5kqShg8froaGBscb2Zo4caJefvll\nSVI0GvX9STtWrFihgoICZWZmul7F3Ndff63Bgwdr7ty5evHFFzVu3DjXK5nJyclROBxWJBJRa2ur\ns9OaWsvOzlZVVVXn7ePHj+vBBx+UJD388MM6ePCgq9Xi7ubnunLlSt17772S/jrhQu/evV2tZuLm\n53vp0iWtXLlSJSUlcZ3j/F9Ga2ur0tPTO28nJiaqvb3dt/9o+/btK+mv511UVKR58+Y53sjOtm3b\n1L9/f+Xm5mrNmjWu1zF36dIlnTt3TtXV1WpsbNTs2bO1a9cuZ+fJtZSWlqazZ8/qscce06VLl1Rd\nXe16JRP5+flqbGzsvB2NRjv/Pvv27avLly+7Wi3ubn6uHd8Yf/fdd9qwYYNqa2tdrWbixucbDoe1\nePFiLVq0KO7fTDg/kk1PT9eVK1c6b0ciEd8GtsP58+c1Y8YMTZ48WZMmTXK9jpmtW7fq4MGDCgaD\nOnHihIqLi9XU1OR6LTMZGRkaM2aMUlJSNGjQIPXu3Vu///6767VMfPDBBxozZozq6uq0fft2LVy4\n8P+97eZXvXr953+ZV65cUb9+/RxuY+/zzz/XkiVLtGbNGvXv39/1OmaOHz+uM2fOqKKiQvPnz9fP\nP/+spUuXxuWxnddsxIgR2rdvnx5//HEdPXpUgwcPdr2SqebmZs2cOVPl5eUaPXq063VM3fidbzAY\nVEVFhQKBgMONbN1///1av369nnvuOf3222+6evWqMjIyXK9lol+/fkpOTpYk3XHHHWpvb7/lVUj8\n5L777tOhQ4c0atQo7d+/Xw899JDrlcxs375dmzdvVk1NjW+/jjsMGzZMn332mSSpsbFR8+fP1+LF\ni+Py2M4jm5eXpwMHDqigoEDRaFTLli1zvZKp6upqtbS0aNWqVVq1apWkv34Afzt8MMjvHnnkEX3z\nzTeaMmWKotGoysvLffsz92effVYlJSUqLCxUKBTSK6+8orS0NNdrmSsuLlZZWZlWrlypQYMGKT8/\n3/VKJsLhsJYuXaoBAwbopZdekiQ98MADKioqcrxZz8MFAgAAMOL8Z7IAAPgVkQUAwAiRBQDACJEF\nAMAIkQUAwAiRBQDACJEFAMDI/wG8y5XLnvMpxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2220a9c5b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer = model.layers[0]\n",
    "weights = layer.get_weights()\n",
    "\n",
    "filterTiles = np.array([])\n",
    "index = 0\n",
    "for i in range(5):\n",
    "    filterRow = []\n",
    "    for j in range(2):\n",
    "        w = weights[0][:,index]\n",
    "        w = w.reshape(3,3)\n",
    "        filterRow.extend(w)\n",
    "        index+=1\n",
    "    filterRow = np.array(filterRow)\n",
    "    if not filterTiles.shape[0] == 0:\n",
    "        filterTiles = np.hstack([filterTiles, filterRow])\n",
    "    else:\n",
    "        filterTiles = filterRow\n",
    "\n",
    "plt.imshow(filterTiles, cmap='gray')    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
